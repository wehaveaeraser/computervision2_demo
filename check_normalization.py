"""
ì¶œë ¥Â·ì •ê·œí™” ì ê²€ ìŠ¤í¬ë¦½íŠ¸

1. ë°ì´í„° ì…ì¶œë ¥ íŒŒì´í”„ë¼ì¸ í™•ì¸: train/val/test ëª¨ë‘ ê°™ì€ ì •ê·œí™” ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
2. ëª¨ë¸ ì¶œë ¥ì˜ activation í™•ì¸: ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ activationê³¼ ì—­ìŠ¤ì¼€ì¼ í™•ì¸
3. ë¹ ë¥¸ ì‹¤í—˜: ëª¨ë¸ ì¶œë ¥ ì´ë¯¸ì§€ì˜ í”½ì…€ íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë ¤ì„œ ë¹„êµ
"""
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
from torch.utils.data import DataLoader
import cv2
import torch.utils.data

# ëª¨ë¸ import
from cnn_model import CNNModel, ImageDataset
from unet_model import UNet


def check_data_normalization(dataset, split_name="Dataset", num_samples=5):
    """
    ë°ì´í„°ì…‹ì˜ ì •ê·œí™” ìƒíƒœ í™•ì¸
    """
    print(f"\n[{split_name}] ì •ê·œí™” ì ê²€:")
    print("-" * 60)
    
    # ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ì˜ í”½ì…€ ê°’ ë²”ìœ„ í™•ì¸
    pixel_ranges = []
    pixel_means = []
    pixel_stds = []
    
    for i in range(min(num_samples, len(dataset))):
        noisy, clean = dataset[i]
        
        # Tensorë¥¼ numpyë¡œ ë³€í™˜ (CHW -> HWC)
        noisy_np = noisy.numpy().transpose(1, 2, 0)
        clean_np = clean.numpy().transpose(1, 2, 0)
        
        # í”½ì…€ ê°’ ë²”ìœ„ í™•ì¸
        noisy_min, noisy_max = noisy_np.min(), noisy_np.max()
        clean_min, clean_max = clean_np.min(), clean_np.max()
        
        pixel_ranges.append({
            'noisy': (noisy_min, noisy_max),
            'clean': (clean_min, clean_max)
        })
        
        pixel_means.append({
            'noisy': noisy_np.mean(),
            'clean': clean_np.mean()
        })
        
        pixel_stds.append({
            'noisy': noisy_np.std(),
            'clean': clean_np.std()
        })
        
        print(f"  ìƒ˜í”Œ {i+1}: Noisy[min={noisy_min:.4f}, max={noisy_max:.4f}, mean={noisy_np.mean():.4f}], "
              f"Clean[min={clean_min:.4f}, max={clean_max:.4f}, mean={clean_np.mean():.4f}]")
    
    # ì •ê·œí™” ì¼ê´€ì„± í™•ì¸
    all_in_range = all(
        (0.0 <= r['noisy'][0] <= 1.0 and 0.0 <= r['noisy'][1] <= 1.0 and
         0.0 <= r['clean'][0] <= 1.0 and 0.0 <= r['clean'][1] <= 1.0)
        for r in pixel_ranges
    )
    
    if all_in_range:
        print(f"  âœ… [{split_name}] ì •ê·œí™” í™•ì¸: ëª¨ë“  ì´ë¯¸ì§€ê°€ [0, 1] ë²”ìœ„ì— ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"  âš ï¸  [{split_name}] ê²½ê³ : ì¼ë¶€ ì´ë¯¸ì§€ê°€ [0, 1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤!")
    
    return pixel_ranges, pixel_means, pixel_stds


def check_train_val_test_normalization(full_dataset, val_split=0.2, num_samples=3):
    """
    Train/Val/Test ëª¨ë‘ì˜ ì •ê·œí™” ìƒíƒœ í™•ì¸
    """
    print("=" * 60)
    print("1. ë°ì´í„° ì •ê·œí™” ì ê²€ (Train/Val/Test ëª¨ë‘ í™•ì¸)")
    print("=" * 60)
    
    # Train/Val ë¶„í•  (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
    val_size = int(len(full_dataset) * val_split)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size], 
        generator=torch.Generator().manual_seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•´ seed ê³ ì •
    )
    
    # TestëŠ” Valê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ë³„ë„ test setì´ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” valì„ testë¡œ ì‚¬ìš©)
    test_dataset = val_dataset
    
    print(f"\në°ì´í„°ì…‹ í¬ê¸°:")
    print(f"  Train: {len(train_dataset)}ê°œ")
    print(f"  Val: {len(val_dataset)}ê°œ")
    print(f"  Test: {len(test_dataset)}ê°œ")
    print(f"  ì´í•©: {len(full_dataset)}ê°œ")
    
    # ê° splitë³„ë¡œ ì •ê·œí™” í™•ì¸
    train_ranges, train_means, train_stds = check_data_normalization(train_dataset, "Train", num_samples)
    val_ranges, val_means, val_stds = check_data_normalization(val_dataset, "Val", num_samples)
    test_ranges, test_means, test_stds = check_data_normalization(test_dataset, "Test", num_samples)
    
    # Train/Val/Test ê°„ ì •ê·œí™” ì¼ê´€ì„± í™•ì¸
    print(f"\n" + "-" * 60)
    print("Train/Val/Test ì •ê·œí™” ì¼ê´€ì„± í™•ì¸:")
    
    # í‰ê·  í”½ì…€ ê°’ ë¹„êµ
    train_noisy_mean = np.mean([m['noisy'] for m in train_means])
    val_noisy_mean = np.mean([m['noisy'] for m in val_means])
    test_noisy_mean = np.mean([m['noisy'] for m in test_means])
    
    train_clean_mean = np.mean([m['clean'] for m in train_means])
    val_clean_mean = np.mean([m['clean'] for m in val_means])
    test_clean_mean = np.mean([m['clean'] for m in test_means])
    
    print(f"  Noisy í‰ê· : Train={train_noisy_mean:.4f}, Val={val_noisy_mean:.4f}, Test={test_noisy_mean:.4f}")
    print(f"  Clean í‰ê· : Train={train_clean_mean:.4f}, Val={val_clean_mean:.4f}, Test={test_clean_mean:.4f}")
    
    # ëª¨ë“  splitì´ ê°™ì€ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸ (ê°™ì€ ImageDatasetì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë™ì¼í•´ì•¼ í•¨)
    all_same_normalization = (
        all(0.0 <= r['noisy'][0] <= 1.0 and 0.0 <= r['noisy'][1] <= 1.0 
            for r in train_ranges + val_ranges + test_ranges) and
        all(0.0 <= r['clean'][0] <= 1.0 and 0.0 <= r['clean'][1] <= 1.0 
            for r in train_ranges + val_ranges + test_ranges)
    )
    
    if all_same_normalization:
        print(f"\nâœ… Train/Val/Test ëª¨ë‘ ë™ì¼í•œ ì •ê·œí™”(/255.0 â†’ [0,1])ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!")
    else:
        print(f"\nâš ï¸  ê²½ê³ : Train/Val/Test ê°„ ì •ê·œí™”ê°€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    
    return train_dataset, val_dataset, test_dataset


def check_model_output_activation(model, device='cuda'):
    """
    ëª¨ë¸ ì¶œë ¥ì˜ activation í™•ì¸
    """
    print("\n" + "=" * 60)
    print("2. ëª¨ë¸ ì¶œë ¥ Activation ì ê²€")
    print("=" * 60)
    
    # ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ í™•ì¸
    model.eval()
    model.to(device)
    
    # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, 3ì±„ë„, 512x512)
    dummy_input = torch.randn(1, 3, 512, 512).to(device)
    
    with torch.no_grad():
        output = model(dummy_input)
    
    # ì¶œë ¥ ë²”ìœ„ í™•ì¸
    output_min = output.min().item()
    output_max = output.max().item()
    output_mean = output.mean().item()
    output_std = output.std().item()
    
    print(f"\nëª¨ë¸ ì¶œë ¥ í†µê³„:")
    print(f"  Min: {output_min:.4f}")
    print(f"  Max: {output_max:.4f}")
    print(f"  Mean: {output_mean:.4f}")
    print(f"  Std: {output_std:.4f}")
    
    # ëª¨ë¸ êµ¬ì¡°ì—ì„œ ë§ˆì§€ë§‰ activation í™•ì¸
    model_name = model.__class__.__name__
    print(f"\nëª¨ë¸: {model_name}")
    
    if model_name == 'CNNModel':
        # CNNModelì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ í™•ì¸
        last_layer = model.conv8
        print(f"  ë§ˆì§€ë§‰ Conv ë ˆì´ì–´: {last_layer}")
        # forwardì—ì„œ clamp ì‚¬ìš© í™•ì¸
        print(f"  âœ… Forwardì—ì„œ torch.clamp() ì‚¬ìš© â†’ ì¶œë ¥ ë²”ìœ„: [0, 1]")
        print(f"\n  ğŸ“ ëª¨ë¸ ë§ˆì§€ë§‰ activation ì½”ë“œ:")
        print(f"     out = self.conv8(x7)")
        print(f"     out = torch.clamp(out, 0.0, 1.0)  # cnn_model.py:64-65")
        print(f"     â†’ clamp ì‚¬ìš© ì¤‘ (sigmoid ëŒ€ì‹  clampë¡œ ë³€ê²½ë¨)")
    elif model_name == 'UNet':
        # UNetì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ í™•ì¸
        last_layer = model.final_conv
        print(f"  ë§ˆì§€ë§‰ Conv ë ˆì´ì–´: {last_layer}")
        # forwardì—ì„œ clamp ì‚¬ìš© í™•ì¸
        print(f"  âœ… Forwardì—ì„œ torch.clamp() ì‚¬ìš© â†’ ì¶œë ¥ ë²”ìœ„: [0, 1]")
        print(f"\n  ğŸ“ ëª¨ë¸ ë§ˆì§€ë§‰ activation ì½”ë“œ:")
        print(f"     out = self.final_conv(dec1)")
        print(f"     out = torch.clamp(out, 0.0, 1.0)  # unet_model.py:96-97")
        print(f"     â†’ clamp ì‚¬ìš© ì¤‘ (sigmoid ëŒ€ì‹  clampë¡œ ë³€ê²½ë¨)")
    
    # ì¶œë ¥ ë²”ìœ„ ê²€ì¦
    if 0.0 <= output_min and output_max <= 1.0:
        print(f"\nâœ… ì¶œë ¥ ë²”ìœ„ í™•ì¸: [0, 1] ë²”ìœ„ì— ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâš ï¸  ê²½ê³ : ì¶œë ¥ì´ [0, 1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤!")
        print(f"   ì˜ˆìƒ ë²”ìœ„: [0, 1], ì‹¤ì œ ë²”ìœ„: [{output_min:.4f}, {output_max:.4f}]")
    
    return {
        'min': output_min,
        'max': output_max,
        'mean': output_mean,
        'std': output_std
    }


def plot_pixel_histograms(model, test_loader, device='cuda', num_samples=3, save_dir='normalization_check'):
    """
    ì…ë ¥/ì¶œë ¥/GT ì´ë¯¸ì§€ì˜ í”½ì…€ íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë ¤ì„œ ë¹„êµ
    """
    print("\n" + "=" * 60)
    print("3. í”½ì…€ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„")
    print("=" * 60)
    
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    model.eval()
    model.to(device)
    
    sample_count = 0
    
    with torch.no_grad():
        for batch_idx, (noisy, clean) in enumerate(test_loader):
            if sample_count >= num_samples:
                break
            
            noisy = noisy.to(device)
            clean = clean.to(device)
            
            # ëª¨ë¸ ì¶œë ¥ (activation ì´í›„ ê°’ ì‚¬ìš© - sigmoidê°€ ëª¨ë¸ ë‚´ë¶€ì— ìˆìŒ)
            output = model(noisy)
            # âœ… ëª¨ë¸ ì¶œë ¥ì— activation ì´í›„ ê°’ ì‚¬ìš© ì¤‘ (sigmoidê°€ forwardì— í¬í•¨ë¨)
            
            batch_size = noisy.size(0)
            for i in range(batch_size):
                if sample_count >= num_samples:
                    break
                
                # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜ (detach ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€)
                noisy_np = noisy[i].detach().cpu().numpy().transpose(1, 2, 0)  # HWC
                clean_np = clean[i].detach().cpu().numpy().transpose(1, 2, 0)  # HWC
                output_np = output[i].detach().cpu().numpy().transpose(1, 2, 0)  # HWC
                
                # í´ë¦¬í•‘ (ì•ˆì „ì„ ìœ„í•´)
                noisy_np = np.clip(noisy_np, 0, 1)
                clean_np = np.clip(clean_np, 0, 1)
                output_np = np.clip(output_np, 0, 1)
                
                # íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° (RGB ì±„ë„ë³„)
                fig, axes = plt.subplots(2, 3, figsize=(18, 12))
                
                colors = ['red', 'green', 'blue']
                channel_names = ['R', 'G', 'B']
                
                # ê° ì±„ë„ë³„ íˆìŠ¤í† ê·¸ë¨
                for ch_idx in range(3):
                    # ì…ë ¥ (Noisy)
                    noisy_flat = noisy_np[:, :, ch_idx].flatten()
                    axes[0, ch_idx].hist(noisy_flat, bins=50, 
                                        range=(0, 1), alpha=0.7, color=colors[ch_idx], 
                                        label=f'Noisy {channel_names[ch_idx]}')
                    axes[0, ch_idx].set_title(f'Input (Noisy) - {channel_names[ch_idx]} Channel')
                    axes[0, ch_idx].set_xlabel('Pixel Value')
                    axes[0, ch_idx].set_ylabel('Frequency')
                    axes[0, ch_idx].set_xlim(0, 1)
                    axes[0, ch_idx].grid(True, alpha=0.3)
                    axes[0, ch_idx].legend()
                    
                    # ì¶œë ¥ (Model Output) vs GT - ê°™ì€ ìŠ¤ì¼€ì¼ë¡œ ë¹„êµ
                    output_flat = output_np[:, :, ch_idx].flatten()
                    clean_flat = clean_np[:, :, ch_idx].flatten()
                    axes[1, ch_idx].hist(output_flat, bins=50, 
                                        range=(0, 1), alpha=0.7, color=colors[ch_idx], 
                                        label=f'Output {channel_names[ch_idx]}')
                    axes[1, ch_idx].hist(clean_flat, bins=50, 
                                        range=(0, 1), alpha=0.5, color='gray', 
                                        label=f'GT {channel_names[ch_idx]}', linestyle='--')
                    axes[1, ch_idx].set_title(f'Output vs GT - {channel_names[ch_idx]} Channel')
                    axes[1, ch_idx].set_xlabel('Pixel Value')
                    axes[1, ch_idx].set_ylabel('Frequency')
                    axes[1, ch_idx].set_xlim(0, 1)
                    axes[1, ch_idx].grid(True, alpha=0.3)
                    axes[1, ch_idx].legend()
                
                # ì „ì²´ ë¹„êµ íˆìŠ¤í† ê·¸ë¨ ì¶”ê°€ (3ê°œë¥¼ í•œ ê·¸ë˜í”„ì—)
                fig2, ax = plt.subplots(1, 1, figsize=(12, 6))
                # ì „ì²´ ì±„ë„ í‰ê· ìœ¼ë¡œ ë¹„êµ
                noisy_all = noisy_np.flatten()
                output_all = output_np.flatten()
                clean_all = clean_np.flatten()
                
                ax.hist(noisy_all, bins=50, range=(0, 1), alpha=0.5, color='blue', 
                       label='Input (Noisy)', density=True)
                ax.hist(output_all, bins=50, range=(0, 1), alpha=0.7, color='red', 
                       label='Output (Model)', density=True)
                ax.hist(clean_all, bins=50, range=(0, 1), alpha=0.5, color='green', 
                       label='GT (Clean)', density=True)
                ax.set_title(f'Sample {sample_count+1} - ì „ì²´ í”½ì…€ íˆìŠ¤í† ê·¸ë¨ ë¹„êµ (ê°™ì€ ìŠ¤ì¼€ì¼ [0,1])')
                ax.set_xlabel('Pixel Value')
                ax.set_ylabel('Density')
                ax.set_xlim(0, 1)
                ax.grid(True, alpha=0.3)
                ax.legend()
                plt.tight_layout()
                plt.savefig(f'{save_dir}/histogram_combined_sample_{sample_count+1}.png', dpi=150, bbox_inches='tight')
                plt.close()
                
                # í†µê³„ ì •ë³´ ì¶”ê°€
                stats_text = f"""
Sample {sample_count+1} Statistics:

Input (Noisy):
  Mean: {noisy_np.mean():.4f}
  Std: {noisy_np.std():.4f}
  Min: {noisy_np.min():.4f}
  Max: {noisy_np.max():.4f}

Output:
  Mean: {output_np.mean():.4f}
  Std: {output_np.std():.4f}
  Min: {output_np.min():.4f}
  Max: {output_np.max():.4f}

GT (Clean):
  Mean: {clean_np.mean():.4f}
  Std: {clean_np.std():.4f}
  Min: {clean_np.min():.4f}
  Max: {clean_np.max():.4f}
"""
                
                # í†µê³„ ì¶œë ¥
                print(f"\nìƒ˜í”Œ {sample_count+1} í†µê³„:")
                print(f"  Input (Noisy): mean={noisy_np.mean():.4f}, std={noisy_np.std():.4f}, "
                      f"min={noisy_np.min():.4f}, max={noisy_np.max():.4f}")
                print(f"  Output: mean={output_np.mean():.4f}, std={output_np.std():.4f}, "
                      f"min={output_np.min():.4f}, max={output_np.max():.4f}")
                print(f"  GT (Clean): mean={clean_np.mean():.4f}, std={clean_np.std():.4f}, "
                      f"min={clean_np.min():.4f}, max={clean_np.max():.4f}")
                
                # ì¶œë ¥ì´ ë°ì€ ìª½ì— ëª°ë ¤ìˆëŠ”ì§€ í™•ì¸ (íˆìŠ¤í† ê·¸ë¨ í•´ì„ ê¸°ì¤€)
                output_mean = output_np.mean()
                clean_mean = clean_np.mean()
                output_max = output_np.max()
                clean_max = clean_np.max()
                
                # íˆìŠ¤í† ê·¸ë¨ í•´ì„ ê¸°ì¤€ ì¶œë ¥
                print(f"\n  ğŸ“Š íˆìŠ¤í† ê·¸ë¨ í•´ì„ ê¸°ì¤€:")
                print(f"     - ì •ìƒ: outputê³¼ GT ë¶„í¬ê°€ ë¹„ìŠ·í•œ êµ¬ê°„ (0.2~0.8)")
                print(f"     - ë¬¸ì œ: outputì´ 0.8~1.0 êµ¬ê°„ì— ëª°ë ¤ìˆê³  GTëŠ” 0.2~0.8")
                
                # ë¬¸ì œ í™•ì • ì¼€ì´ìŠ¤ ì²´í¬
                output_high_range_ratio = np.sum((output_np > 0.8) & (output_np <= 1.0)) / output_np.size
                clean_high_range_ratio = np.sum((clean_np > 0.8) & (clean_np <= 1.0)) / clean_np.size
                
                if output_mean > clean_mean + 0.1:  # 0.1 ì´ìƒ ì°¨ì´
                    print(f"\n  âš ï¸  [ë¬¸ì œ í™•ì •] ì¶œë ¥ì´ GTë³´ë‹¤ ë°ìŠµë‹ˆë‹¤!")
                    print(f"     - í‰ê·  ì°¨ì´: {output_mean - clean_mean:.4f}")
                    print(f"     - ì¶œë ¥ í‰ê· : {output_mean:.4f}, GT í‰ê· : {clean_mean:.4f}")
                    print(f"     - ì¶œë ¥ 0.8~1.0 ë¹„ìœ¨: {output_high_range_ratio*100:.1f}%, GT: {clean_high_range_ratio*100:.1f}%")
                    if output_high_range_ratio > 0.3:  # 30% ì´ìƒì´ ë°ì€ êµ¬ê°„ì— ìˆìœ¼ë©´
                        print(f"     â†’ âœ… 'ì¶œë ¥ ìŠ¤ì¼€ì¼ ì˜¤ë¥˜ or activation ë¯¸ë³´ì •' 100% í™•ì •!")
                        print(f"     â†’ í•´ê²°: ëª¨ë¸ ë§ˆì§€ë§‰ activation ì½”ë“œ í™•ì¸ í•„ìš”")
                elif output_mean < clean_mean - 0.1:
                    print(f"\n  âš ï¸  ê²½ê³ : ì¶œë ¥ì´ GTë³´ë‹¤ ì–´ë‘¡ìŠµë‹ˆë‹¤! (ì°¨ì´: {clean_mean - output_mean:.4f})")
                else:
                    print(f"\n  âœ… [ì •ìƒ] ì¶œë ¥ê³¼ GTì˜ ë°ê¸°ê°€ ë¹„ìŠ·í•©ë‹ˆë‹¤.")
                    print(f"     - ì¶œë ¥ í‰ê· : {output_mean:.4f}, GT í‰ê· : {clean_mean:.4f}")
                    print(f"     â†’ ì •ê·œí™”ëŠ” ì •ìƒ â†’ ë‹¤ìŒ ì›ì¸ì€ Loss / ëª¨ë¸ êµ¬ì¡° ìª½ í™•ì¸ í•„ìš”")
                
                plt.suptitle(f'Sample {sample_count+1} - Pixel Histograms', fontsize=16, y=0.995)
                plt.tight_layout()
                plt.savefig(f'{save_dir}/histogram_sample_{sample_count+1}.png', dpi=150, bbox_inches='tight')
                plt.close()
                
                # ì´ë¯¸ì§€ ë¹„êµë„ ì €ì¥
                fig, axes = plt.subplots(1, 3, figsize=(15, 5))
                axes[0].imshow(noisy_np)
                axes[0].set_title(f'Input (Noisy) #{sample_count+1}')
                axes[0].axis('off')
                
                axes[1].imshow(output_np)
                axes[1].set_title(f'Output #{sample_count+1}')
                axes[1].axis('off')
                
                axes[2].imshow(clean_np)
                axes[2].set_title(f'GT (Clean) #{sample_count+1}')
                axes[2].axis('off')
                
                plt.tight_layout()
                plt.savefig(f'{save_dir}/comparison_sample_{sample_count+1}.png', dpi=150, bbox_inches='tight')
                plt.close()
                
                sample_count += 1
                print(f"  âœ… íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ: {save_dir}/histogram_sample_{sample_count}.png")
    
    print(f"\nâœ… íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ {save_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    parser = argparse.ArgumentParser(description='ì¶œë ¥Â·ì •ê·œí™” ì ê²€ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--model_path', type=str, default=None,
                        help='í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì—†ìœ¼ë©´ ë°ì´í„° ì •ê·œí™”ë§Œ ì ê²€)')
    parser.add_argument('--model_type', type=str, choices=['cnn', 'unet'], default=None,
                        help='ëª¨ë¸ íƒ€ì… (cnn ë˜ëŠ” unet, --model_pathê°€ ìˆì„ ë•Œ í•„ìˆ˜)')
    parser.add_argument('--noisy_dir', type=str, required=True,
                        help='ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬')
    parser.add_argument('--clean_dir', type=str, default=None,
                        help='ê¹¨ë—í•œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ (Noneì´ë©´ noisy_dirì—ì„œ íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ë§¤ì¹­)')
    parser.add_argument('--device', type=str, default='cuda',
                        help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (cuda/cpu)')
    parser.add_argument('--gpu_id', type=int, default=0,
                        help='ì‚¬ìš©í•  GPU ID (default: 0)')
    parser.add_argument('--num_samples', type=int, default=3,
                        help='íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ì— ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜ (default: 3)')
    parser.add_argument('--save_dir', type=str, default='normalization_check',
                        help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (default: normalization_check)')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ê²½ë¡œê°€ ìˆìœ¼ë©´ model_typeë„ í•„ìš”
    if args.model_path and not args.model_type:
        parser.error("--model_pathê°€ ì œê³µë˜ë©´ --model_typeë„ í•„ìš”í•©ë‹ˆë‹¤ (cnn ë˜ëŠ” unet)")
    
    # GPU ì„¤ì •
    if args.device == 'cuda':
        if torch.cuda.is_available():
            device = torch.device(f'cuda:{args.gpu_id}')
            torch.cuda.set_device(args.gpu_id)
            print(f'GPU ì‚¬ìš©: {torch.cuda.get_device_name(args.gpu_id)}')
        else:
            print('âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.')
            device = torch.device('cpu')
    else:
        device = torch.device('cpu')
    
    print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\n')
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print('ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...')
    full_dataset = ImageDataset(args.noisy_dir, args.clean_dir)
    print(f'ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {len(full_dataset)}ê°œ ì´ë¯¸ì§€ ìŒ\n')
    
    # 1. Train/Val/Test ëª¨ë‘ì˜ ì •ê·œí™” ì ê²€ (í•­ìƒ ì‹¤í–‰)
    train_dataset, val_dataset, test_dataset = check_train_val_test_normalization(
        full_dataset, val_split=0.2, num_samples=args.num_samples
    )
    
    # Test loader ìƒì„± (ëª¨ë¸ì´ ìˆì„ ë•Œ ì‚¬ìš©)
    test_loader = DataLoader(
        test_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=0
    )
    
    # ëª¨ë¸ì´ ìˆìœ¼ë©´ ì¶”ê°€ ì ê²€ ìˆ˜í–‰
    if args.model_path:
        # ëª¨ë¸ ë¡œë“œ
        print('ëª¨ë¸ ë¡œë”© ì¤‘...')
        if args.model_type == 'cnn':
            model = CNNModel(in_channels=3, out_channels=3)
        else:
            model = UNet(in_channels=3, out_channels=3)
        
        checkpoint = torch.load(args.model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        print('ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n')
        
        # 2. ëª¨ë¸ ì¶œë ¥ activation ì ê²€
        check_model_output_activation(model, device=device)
        
        # 3. í”½ì…€ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
        plot_pixel_histograms(model, test_loader, device=device, 
                             num_samples=args.num_samples, save_dir=args.save_dir)
        
        print("\n" + "=" * 60)
        print("ì ê²€ ì™„ë£Œ!")
        print("=" * 60)
        print("\nì ê²€ ê²°ê³¼ ìš”ì•½:")
        print("1. ë°ì´í„° ì •ê·œí™”: ImageDatasetì—ì„œ /255.0ìœ¼ë¡œ [0,1] ë²”ìœ„ë¡œ ì •ê·œí™”")
        print("2. ëª¨ë¸ ì¶œë ¥: sigmoid activationìœ¼ë¡œ [0,1] ë²”ìœ„ ì¶œë ¥")
        print("3. íˆìŠ¤í† ê·¸ë¨: ì…ë ¥/ì¶œë ¥/GTì˜ í”½ì…€ ë¶„í¬ ë¹„êµ")
        print(f"\nê²°ê³¼ íŒŒì¼ì€ {args.save_dir} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\n" + "=" * 60)
        print("ë°ì´í„° ì •ê·œí™” ì ê²€ ì™„ë£Œ!")
        print("=" * 60)
        print("\nì ê²€ ê²°ê³¼ ìš”ì•½:")
        print("1. ë°ì´í„° ì •ê·œí™”: ImageDatasetì—ì„œ /255.0ìœ¼ë¡œ [0,1] ë²”ìœ„ë¡œ ì •ê·œí™”")
        print("\nğŸ’¡ ëª¨ë¸ì´ ìˆìœ¼ë©´ --model_pathì™€ --model_typeì„ ì¶”ê°€í•˜ì—¬")
        print("   ëª¨ë¸ ì¶œë ¥ activationê³¼ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ë„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()

