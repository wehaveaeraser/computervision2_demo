"""
CNN ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
import torch
from torch.utils.data import DataLoader
from cnn_model import CNNModel, ImageDataset, train_model, evaluate_model
import argparse
from pathlib import Path


def main():
    parser = argparse.ArgumentParser(description='CNN ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--noisy_dir', type=str, required=True,
                        help='ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ë˜ëŠ” ì •ì œëœ ë°ì´í„°ê°€ ìˆëŠ” í´ë”)')
    parser.add_argument('--clean_dir', type=str, default=None,
                        help='ê¹¨ë—í•œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ (Noneì´ë©´ noisy_dirì—ì„œ íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ë§¤ì¹­)')
    parser.add_argument('--matched_pairs_file', type=str, default=None,
                        help='ë§¤ì¹­ëœ ì´ë¯¸ì§€ ìŒ ì •ë³´ê°€ ë‹´ê¸´ JSON íŒŒì¼ ê²½ë¡œ (í´ë”ëª… ê¸°ë°˜ ë§¤ì¹­ ê²°ê³¼)')
    parser.add_argument('--auto_match', action='store_true',
                        help='í•™ìŠµ ì‹œ ìë™ìœ¼ë¡œ í´ë”ëª… ê¸°ë°˜ ë§¤ì¹­ ìˆ˜í–‰ (matched_pairs_file ì—†ì´ ì‚¬ìš© ê°€ëŠ¥)')
    parser.add_argument('--l1_weight', type=float, default=1.0,
                        help='L1 Loss ê°€ì¤‘ì¹˜ (default: 1.0)')
    parser.add_argument('--ssim_weight', type=float, default=1.0,
                        help='SSIM Loss ê°€ì¤‘ì¹˜ (default: 1.0)')
    parser.add_argument('--gradient_weight', type=float, default=0.5,
                        help='Gradient Loss ê°€ì¤‘ì¹˜ (default: 0.5)')
    parser.add_argument('--batch_size', type=int, default=2,
                        help='ë°°ì¹˜ í¬ê¸° (default: 2, Mixed Precision ì—†ì„ ë•Œ ë©”ëª¨ë¦¬ ì ˆì•½)')
    parser.add_argument('--lr', type=float, default=5e-5,
                        help='í•™ìŠµë¥  (default: 5e-5)')
    parser.add_argument('--epochs', type=int, default=50,
                        help='ì—í¬í¬ ìˆ˜ (default: 50)')
    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints/cnn',
                        help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬ (default: checkpoints/cnn)')
    parser.add_argument('--train_ratio', type=float, default=0.7,
                        help='í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (default: 0.7)')
    parser.add_argument('--val_ratio', type=float, default=0.15,
                        help='ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (default: 0.15)')
    parser.add_argument('--test_ratio', type=float, default=0.15,
                        help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (default: 0.15)')
    parser.add_argument('--device', type=str, default='cuda',
                        help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (cuda/cpu) (default: cuda)')
    parser.add_argument('--gpu_id', type=int, default=0,
                        help='ì‚¬ìš©í•  GPU ID (default: 0)')
    parser.add_argument('--resume', type=str, default=None,
                        help='ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (í•™ìŠµ ì¬ê°œìš©, ì˜ˆ: checkpoints/cnn/best_model.pth)')
    
    args = parser.parse_args()
    
    # GPU ì„¤ì • ë° í™•ì¸
    if args.device == 'cuda':
        if torch.cuda.is_available():
            device = torch.device(f'cuda:{args.gpu_id}')
            torch.cuda.set_device(args.gpu_id)
            print(f'=' * 50)
            print(f'GPU ì‚¬ìš© ê°€ëŠ¥!')
            print(f'GPU ì¥ì¹˜: {torch.cuda.get_device_name(args.gpu_id)}')
            print(f'GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(args.gpu_id).total_memory / 1024**3:.2f} GB')
            print(f'CUDA ë²„ì „: {torch.version.cuda}')
            print(f'PyTorch ë²„ì „: {torch.__version__}')
            print(f'=' * 50)
        else:
            print(' ê²½ê³ : CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.')
            device = torch.device('cpu')
    else:
        device = torch.device('cpu')
        print(f'CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.')
    
    print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print('ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...')
    if args.matched_pairs_file is not None:
        print(f'ë§¤ì¹­ íŒŒì¼ ëª¨ë“œ: {args.matched_pairs_file}ì—ì„œ ë§¤ì¹­ ì •ë³´ë¥¼ ì½ìŠµë‹ˆë‹¤.')
        print(f'  noisy_dir={args.noisy_dir}, clean_dir={args.clean_dir}')
    elif args.clean_dir is None:
        print(f'ë‹¨ì¼ í´ë” ëª¨ë“œ: {args.noisy_dir}ì—ì„œ íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.')
    else:
        print(f'ì´ì¤‘ í´ë” ëª¨ë“œ: noisy_dir={args.noisy_dir}, clean_dir={args.clean_dir}')
    
    # train/val/test í´ë” êµ¬ì¡° ìë™ ê°ì§€
    def has_split_structure(base_dir):
        """í´ë”ì— train/val/test êµ¬ì¡°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        base_path = Path(base_dir)
        train_exists = (base_path / 'train').exists()
        val_exists = (base_path / 'val').exists()
        test_exists = (base_path / 'test').exists()
        return train_exists and val_exists and test_exists
    
    # ë…¸ì´ì¦ˆ ë°ì´í„°ì˜ train/val/test êµ¬ì¡°ë¥¼ ì¡´ì¤‘í•˜ëŠ” ëª¨ë“œ
    # auto_match ëª¨ë“œì´ê±°ë‚˜ í´ë” êµ¬ì¡°ê°€ ìˆìœ¼ë©´ split êµ¬ì¡° ì‚¬ìš©
    use_split_structure = (args.auto_match and args.clean_dir is not None) or \
                          (has_split_structure(args.noisy_dir) and (args.clean_dir is None or has_split_structure(args.clean_dir)))
    
    if use_split_structure:
        # ë…¸ì´ì¦ˆ ë°ì´í„°ì˜ train/val/test êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if args.auto_match and args.clean_dir is not None:
            print("ë…¸ì´ì¦ˆ ë°ì´í„°ì˜ train/val/test êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print("Trainê³¼ Validation ëª¨ë‘ ë…¸ì´ì¦ˆ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.")
            
            # Train ë°ì´í„°ì…‹: ë…¸ì´ì¦ˆ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­
            train_dataset = ImageDataset(
                args.noisy_dir, args.clean_dir,
                matched_pairs_file=args.matched_pairs_file,
                auto_match=True,
                split='train',
                reverse_match=False  # ë…¸ì´ì¦ˆ ë°ì´í„° ê¸°ì¤€
            )
            
            # Validation ë°ì´í„°ì…‹: ë…¸ì´ì¦ˆ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­
            val_dataset = ImageDataset(
                args.noisy_dir, args.clean_dir,
                matched_pairs_file=args.matched_pairs_file,
                auto_match=True,
                split='val',
                reverse_match=False  # ë…¸ì´ì¦ˆ ë°ì´í„° ê¸°ì¤€
            )
            
            # Trainê³¼ Val ë°ì´í„°ë¥¼ í•©ì³ì„œ ë¹„ìœ¨ì— ë§ê²Œ ì¬ë¶„í• 
            # ëª©í‘œ: train:val:test = 7:1.5:1.5
            total_samples = len(train_dataset) + len(val_dataset)
            target_train_ratio = 7.0 / 10.0  # 7 / (7 + 1.5 + 1.5)
            target_val_ratio = 1.5 / 10.0
            target_test_ratio = 1.5 / 10.0
            
            target_train_size = int(total_samples * target_train_ratio)
            target_val_size = int(total_samples * target_val_ratio)
            target_test_size = total_samples - target_train_size - target_val_size
            
            current_train_size = len(train_dataset)
            current_val_size = len(val_dataset)
            
            print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  ì¡°ì •:")
            print(f"   í˜„ì¬: Train={current_train_size}, Val={current_val_size}, Total={total_samples}")
            print(f"   ëª©í‘œ ë¹„ìœ¨: Train:Val:Test = 7:1.5:1.5")
            print(f"   ëª©í‘œ: Train={target_train_size}, Val={target_val_size}, Test={target_test_size}")
            
            # ì „ì²´ ë°ì´í„°ì…‹ì„ í•©ì³ì„œ ì¬ë¶„í• 
            from torch.utils.data import ConcatDataset
            full_dataset = ConcatDataset([train_dataset, val_dataset])
            
            # ë¹„ìœ¨ì— ë§ê²Œ ë¶„í• 
            train_dataset, val_dataset, _ = torch.utils.data.random_split(
                full_dataset, 
                [target_train_size, target_val_size, target_test_size],
                generator=torch.Generator().manual_seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì •
            )
            
            print(f"   ì¡°ì • í›„: Train={len(train_dataset)}, Val={len(val_dataset)}")
            # test ë°ì´í„°ëŠ” inference ì½”ë“œì—ì„œë§Œ ì‚¬ìš©
        else:
            # í´ë” êµ¬ì¡° ìë™ ê°ì§€ ëª¨ë“œ
            print("train/val/test í´ë” êµ¬ì¡°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            # ê° splitë³„ë¡œ ë°ì´í„°ì…‹ ìƒì„±
            train_dataset = ImageDataset(
                str(Path(args.noisy_dir) / 'train'), 
                str(Path(args.clean_dir) / 'train') if args.clean_dir else None,
                matched_pairs_file=args.matched_pairs_file,
                auto_match=False
            )
            val_dataset = ImageDataset(
                str(Path(args.noisy_dir) / 'val'),
                str(Path(args.clean_dir) / 'val') if args.clean_dir else None,
                matched_pairs_file=args.matched_pairs_file,
                auto_match=False
            )
            # test ë°ì´í„°ëŠ” inference ì½”ë“œì—ì„œë§Œ ì‚¬ìš©
        print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}')
        print('â„¹ï¸  Test ë°ì´í„°ëŠ” inference ì½”ë“œì—ì„œ í‰ê°€í•˜ì„¸ìš”.')
    else:
        # ê¸°ì¡´ ë°©ì‹: ì „ì²´ ë°ì´í„°ì…‹ì„ ë¡œë“œ í›„ ëœë¤ ë¶„í• 
        full_dataset = ImageDataset(args.noisy_dir, args.clean_dir, 
                                    matched_pairs_file=args.matched_pairs_file,
                                    auto_match=args.auto_match)
        
        # Train/Validation ë¶„í•  (testëŠ” inferenceì—ì„œ ì‚¬ìš©)
        total_size = len(full_dataset)
        train_size = int(total_size * args.train_ratio)
        val_size = int(total_size * args.val_ratio)
        # test_sizeëŠ” ê³„ì‚°í•˜ë˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (inferenceì—ì„œ ì‚¬ìš©)
        test_size = total_size - train_size - val_size
        
        train_dataset, val_dataset, _ = torch.utils.data.random_split(
            full_dataset, [train_size, val_size, test_size]
        )
        print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}')
        print(f'ë¶„í•  ë¹„ìœ¨: Train={len(train_dataset)/total_size:.2%}, Val={len(val_dataset)/total_size:.2%}, Test={test_size/total_size:.2%} (inferenceì—ì„œ ì‚¬ìš©)')
        print('â„¹ï¸  Test ë°ì´í„°ëŠ” inference ì½”ë“œì—ì„œ í‰ê°€í•˜ì„¸ìš”.')
    
    # DataLoader ìƒì„±
    # Windows í˜¸í™˜ì„±ì„ ìœ„í•´ num_workers=0 ì‚¬ìš©
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=0,  # Windows í˜¸í™˜ì„±
        pin_memory=True if device.type == 'cuda' else False
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=0,  # Windows í˜¸í™˜ì„±
        pin_memory=True if device.type == 'cuda' else False
    )
    
    
    # ëª¨ë¸ ìƒì„±
    model = CNNModel(in_channels=3, out_channels=3)
    print(f'ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}')
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    start_epoch = 0
    if args.resume:
        print(f'\nì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ: {args.resume}')
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        start_epoch = checkpoint.get('epoch', 0) + 1
        print(f'âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ!')
        print(f'   - Epoch: {checkpoint.get("epoch", 0)}')
        print(f'   - Val Loss: {checkpoint.get("val_loss", "N/A"):.4f}')
        print(f'   - Val PSNR: {checkpoint.get("val_psnr", "N/A"):.4f}')
        print(f'   - Val SSIM: {checkpoint.get("val_ssim", "N/A"):.4f}')
        print(f'   - ë‹¤ìŒ Epochë¶€í„° ì¬ê°œ: {start_epoch}')
    
    # í•™ìŠµ
    print('\ní•™ìŠµ ì‹œì‘...')
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=args.epochs,
        device=device,
        lr=args.lr,
        checkpoint_dir=args.checkpoint_dir,
        start_epoch=start_epoch,
        resume_checkpoint=args.resume,
        l1_weight=args.l1_weight,
        ssim_weight=args.ssim_weight,
        gradient_weight=args.gradient_weight
    )
    
    # ìµœì¢… í‰ê°€
    print('\nìµœì¢… í‰ê°€ ì¤‘...')
    best_model = CNNModel(in_channels=3, out_channels=3)
    checkpoint = torch.load(f'{args.checkpoint_dir}/best_model.pth', map_location=device)
    best_model.load_state_dict(checkpoint['model_state_dict'])
    best_model.to(device)
    
    print('\n=== Validation Set í‰ê°€ ===')
    evaluate_model(best_model, val_loader, device=device)
    
    print('\nâœ… í•™ìŠµ ì™„ë£Œ!')
    print('â„¹ï¸  Test ë°ì´í„° í‰ê°€ëŠ” inference ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:')
    print(f'   python inference_cnn.py --model_path {args.checkpoint_dir}/best_model.pth --noisy_dir {args.noisy_dir} --clean_dir {args.clean_dir if args.clean_dir else ""} --visualize')


if __name__ == '__main__':
    main()

